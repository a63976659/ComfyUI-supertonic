"""
ComfyUI Node for Supertonic TTS
"""

import os
import numpy as np
import torch
import soundfile as sf
from io import BytesIO

# Import helper functions
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
py_dir = os.path.join(current_dir, "py")
if py_dir not in sys.path:
    sys.path.insert(0, py_dir)

from helper import load_text_to_speech, load_voice_style, Style


class SupertonicTTS:
    """
    Supertonic æ–‡æœ¬è½¬è¯­éŸ³èŠ‚ç‚¹
    è¶…å¿«é€Ÿæœ¬åœ° TTSï¼Œæ”¯æŒè‡ªç„¶æ–‡æœ¬å¤„ç†
    """
    
    def __init__(self):
        self.text_to_speech = None
        self.current_model_dir = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # èŽ·å–å¯ç”¨çš„éŸ³è‰²
        voice_styles_dir = os.path.join(current_dir, "assets", "voice_styles")
        voice_styles = []
        voice_map = {}
        
        if os.path.exists(voice_styles_dir):
            files = [f[:-5] for f in os.listdir(voice_styles_dir) if f.endswith('.json')]
            # åˆ›å»ºä¸­æ–‡æ˜ å°„
            for f in files:
                if f == "M1":
                    voice_map["ç”·å£°1"] = "M1"
                elif f == "M2":
                    voice_map["ç”·å£°2"] = "M2"
                elif f == "F1":
                    voice_map["å¥³å£°1"] = "F1"
                elif f == "F2":
                    voice_map["å¥³å£°2"] = "F2"
            voice_styles = list(voice_map.keys())
        
        if not voice_styles:
            voice_styles = ["ç”·å£°1", "ç”·å£°2", "å¥³å£°1", "å¥³å£°2"]
        
        return {
            "required": {
                "è¾“å…¥æ–‡æœ¬": ("STRING", {
                    "multiline": True,
                    "default": "ä»Šå¤©æ—©ä¸Šæˆ‘åœ¨å…¬å›­æ•£æ­¥ï¼Œé¸Ÿé¸£å’Œå¾®é£Žçš„å£°éŸ³è®©äººå¿ƒæ—·ç¥žæ€¡ã€‚"
                }),
                "éŸ³è‰²é€‰æ‹©": (voice_styles, {
                    "default": voice_styles[0] if voice_styles else "ç”·å£°1"
                }),
                "æŽ¨ç†æ­¥æ•°": ("INT", {
                    "default": 5,
                    "min": 1,
                    "max": 20,
                    "step": 1,
                    "display": "slider",
                    "tooltip": "æ­¥æ•°è¶Šå¤šè´¨é‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢ã€‚æŽ¨è: 2(å¿«é€Ÿ) 5(é»˜è®¤) 10(é«˜è´¨é‡)"
                }),
                "è¯­é€Ÿå€æ•°": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "1.0ä¸ºæ­£å¸¸é€Ÿåº¦ï¼Œå¤§äºŽ1.0åŠ å¿«ï¼Œå°äºŽ1.0å‡æ…¢"
                }),
                "å¥é—´åœé¡¿": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "å¥å­ä¹‹é—´çš„åœé¡¿æ—¶é•¿(ç§’)"
                }),
            },
            "optional": {
                "ä½¿ç”¨GPU": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ(å½“å‰å»ºè®®ä½¿ç”¨CPU)"
                }),
                "è¾“å‡ºé‡‡æ ·çŽ‡": (["44100 Hz (åŽŸå§‹)", "48000 Hz (Opuså…¼å®¹)"], {
                    "default": "44100 Hz (åŽŸå§‹)",
                    "tooltip": "å¦‚æžœä½¿ç”¨Opusæ ¼å¼ä¿å­˜ï¼Œè¯·é€‰æ‹©48000 Hzé¿å…å‘éŸ³å’Œæ—¶é•¿é—®é¢˜"
                }),
                "åˆ†å¥ç­–ç•¥": (["æ ‡å‡†ï¼ˆã€‚ï¼ï¼Ÿï¼‰", "å¢žå¼ºï¼ˆã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼šâ€¦â€¦ï¼‰"], {
                    "default": "æ ‡å‡†ï¼ˆã€‚ï¼ï¼Ÿï¼‰",
                    "tooltip": "é€‰æ‹©åˆ†å¥è§„åˆ™ï¼Œå¢žå¼ºæ¨¡å¼ä¼šåœ¨é€—å·ç­‰å¤„ä¹ŸåŠ å…¥åœé¡¿"
                }),
            }
        }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("éŸ³é¢‘è¾“å‡º",)
    FUNCTION = "generate_speech"
    CATEGORY = "ðŸŽ™ï¸ Supertonic è¯­éŸ³åˆæˆ/è¯­éŸ³åˆæˆ"
    
    def generate_speech(self, è¾“å…¥æ–‡æœ¬, éŸ³è‰²é€‰æ‹©, æŽ¨ç†æ­¥æ•°, è¯­é€Ÿå€æ•°, å¥é—´åœé¡¿, ä½¿ç”¨GPU=False, è¾“å‡ºé‡‡æ ·çŽ‡="44100 Hz (åŽŸå§‹)", åˆ†å¥ç­–ç•¥="æ ‡å‡†ï¼ˆã€‚ï¼ï¼Ÿï¼‰"):
        """ä½¿ç”¨ Supertonic TTS ç”Ÿæˆè¯­éŸ³"""
        
        try:
            # ä¸­æ–‡åˆ°è‹±æ–‡éŸ³è‰²æ˜ å°„
            voice_map = {
                "ç”·å£°1": "M1",
                "ç”·å£°2": "M2",
                "å¥³å£°1": "F1",
                "å¥³å£°2": "F2"
            }
            voice_style = voice_map.get(éŸ³è‰²é€‰æ‹©, éŸ³è‰²é€‰æ‹©)
            text = è¾“å…¥æ–‡æœ¬
            total_steps = æŽ¨ç†æ­¥æ•°
            speed = è¯­é€Ÿå€æ•°
            silence_duration = å¥é—´åœé¡¿
            use_gpu = ä½¿ç”¨GPU
            # åˆ†å¥ç­–ç•¥æ˜ å°„
            segmentation_strategy = "standard" if åˆ†å¥ç­–ç•¥.startswith("æ ‡å‡†") else "enhanced"
            
            # Load model if not loaded or model directory changed
            onnx_dir = os.path.join(current_dir, "assets", "onnx")
            
            if not os.path.exists(onnx_dir):
                raise FileNotFoundError(
                    f"ONNX æ¨¡åž‹æœªæ‰¾åˆ°: {onnx_dir}\n"
                    "è¯·ä¸‹è½½æ¨¡åž‹: git clone https://huggingface.co/Supertone/supertonic assets"
                )
            
            if self.text_to_speech is None or self.current_model_dir != onnx_dir:
                self.text_to_speech = load_text_to_speech(onnx_dir, use_gpu)
                self.current_model_dir = onnx_dir
            
            # Load voice style
            voice_style_path = os.path.join(current_dir, "assets", "voice_styles", f"{voice_style}.json")
            
            if not os.path.exists(voice_style_path):
                raise FileNotFoundError(
                    f"éŸ³è‰²æ–‡ä»¶ '{voice_style}' æœªæ‰¾åˆ°: {voice_style_path}\n"
                    "è¯·ç¡®ä¿éŸ³è‰²æ–‡ä»¶å·²ä¸‹è½½åˆ° assets/voice_styles/ ç›®å½•"
                )
            
            style = load_voice_style([voice_style_path], verbose=False)
            
            # ç”Ÿæˆè¯­éŸ³
            import time
            start_time = time.time()
            print(f"[Supertonic TTS] æ–‡æœ¬: {text}")
            
            wav, duration = self.text_to_speech(
                text, 
                style, 
                total_steps, 
                speed,
                silence_duration,
                segmentation_strategy
            )
            
            # Trim to actual duration
            wav_trimmed = wav[0, :int(self.text_to_speech.sample_rate * duration[0].item())]
            
            # éŸ³é¢‘å½’ä¸€åŒ–å¤„ç†ï¼šç¡®ä¿åœ¨ [-1, 1] èŒƒå›´å†…
            max_val = np.abs(wav_trimmed).max()
            if max_val > 1.0:
                wav_trimmed = wav_trimmed / max_val
            elif max_val < 0.01:
                wav_trimmed = wav_trimmed / max_val * 0.5
            
            # å¤„ç†è¾“å‡ºé‡‡æ ·çŽ‡
            target_sample_rate = self.text_to_speech.sample_rate  # é»˜è®¤ 44100
            if "è¾“å‡ºé‡‡æ ·çŽ‡" in locals() and è¾“å‡ºé‡‡æ ·çŽ‡ == "48000 Hz (Opuså…¼å®¹)":
                target_sample_rate = 48000
                # ä½¿ç”¨ torchaudio é‡é‡‡æ ·
                import torchaudio
                wav_tensor = torch.from_numpy(wav_trimmed).float().unsqueeze(0)  # [1, samples]
                wav_resampled = torchaudio.functional.resample(
                    wav_tensor, 
                    self.text_to_speech.sample_rate, 
                    target_sample_rate
                )
                wav_trimmed = wav_resampled.squeeze(0).numpy()
            
            # Convert to ComfyUI audio format
            # ComfyUI expects audio as a dict with 'waveform' and 'sample_rate'
            # waveform shape: [batch, channels, samples]
            waveform = torch.from_numpy(wav_trimmed).float().unsqueeze(0).unsqueeze(0)
            
            audio_output = {
                "waveform": waveform,  # [1, 1, samples] - mono audio
                "sample_rate": target_sample_rate
            }
            
            elapsed_time = time.time() - start_time
            print(f"[Supertonic TTS] ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            
            return (audio_output,)
            
        except Exception as e:
            print(f"[Supertonic TTS] âŒ é”™è¯¯: {str(e)}")
            raise


class SupertonicBatchTTS:
    """
    Supertonic æ‰¹é‡æ–‡æœ¬è½¬è¯­éŸ³èŠ‚ç‚¹
    åŒæ—¶å¤„ç†å¤šä¸ªæ–‡æœ¬ä»¥æé«˜æ•ˆçŽ‡
    """
    
    def __init__(self):
        self.text_to_speech = None
        self.current_model_dir = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # èŽ·å–å¯ç”¨çš„éŸ³è‰²
        voice_styles_dir = os.path.join(current_dir, "assets", "voice_styles")
        voice_styles = []
        voice_map = {}
        
        if os.path.exists(voice_styles_dir):
            files = [f[:-5] for f in os.listdir(voice_styles_dir) if f.endswith('.json')]
            for f in files:
                if f == "M1":
                    voice_map["ç”·å£°1"] = "M1"
                elif f == "M2":
                    voice_map["ç”·å£°2"] = "M2"
                elif f == "F1":
                    voice_map["å¥³å£°1"] = "F1"
                elif f == "F2":
                    voice_map["å¥³å£°2"] = "F2"
            voice_styles = list(voice_map.keys())
        
        if not voice_styles:
            voice_styles = ["ç”·å£°1", "ç”·å£°2", "å¥³å£°1", "å¥³å£°2"]
        
        return {
            "required": {
                "æ–‡æœ¬1": ("STRING", {
                    "multiline": True,
                    "default": "ç¬¬ä¸€æ®µè¦åˆæˆçš„æ–‡æœ¬ã€‚"
                }),
                "éŸ³è‰²1": (voice_styles, {
                    "default": voice_styles[0] if voice_styles else "ç”·å£°1"
                }),
                "æŽ¨ç†æ­¥æ•°": ("INT", {
                    "default": 5,
                    "min": 1,
                    "max": 20,
                    "step": 1,
                    "display": "slider",
                    "tooltip": "æ­¥æ•°è¶Šå¤šè´¨é‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢"
                }),
                "è¯­é€Ÿå€æ•°": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "1.0ä¸ºæ­£å¸¸é€Ÿåº¦"
                }),
            },
            "optional": {
                "æ–‡æœ¬2": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "å¯é€‰çš„ç¬¬äºŒæ®µæ–‡æœ¬"
                }),
                "éŸ³è‰²2": (voice_styles, {
                    "default": voice_styles[0] if voice_styles else "ç”·å£°1"
                }),
                "æ–‡æœ¬3": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "å¯é€‰çš„ç¬¬ä¸‰æ®µæ–‡æœ¬"
                }),
                "éŸ³è‰²3": (voice_styles, {
                    "default": voice_styles[0] if voice_styles else "ç”·å£°1"
                }),
                "ä½¿ç”¨GPU": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ"
                }),
            }
        }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("éŸ³é¢‘åˆ—è¡¨",)
    FUNCTION = "generate_batch_speech"
    CATEGORY = "ðŸŽ™ï¸ Supertonic è¯­éŸ³åˆæˆ/è¯­éŸ³åˆæˆ"
    OUTPUT_IS_LIST = (True,)
    
    def generate_batch_speech(self, æ–‡æœ¬1, éŸ³è‰²1, æŽ¨ç†æ­¥æ•°, è¯­é€Ÿå€æ•°, 
                             æ–‡æœ¬2="", éŸ³è‰²2="ç”·å£°1", 
                             æ–‡æœ¬3="", éŸ³è‰²3="ç”·å£°1",
                             ä½¿ç”¨GPU=False):
        """æ‰¹é‡ç”Ÿæˆå¤šä¸ªæ–‡æœ¬çš„è¯­éŸ³"""
        
        # ä¸­æ–‡åˆ°è‹±æ–‡éŸ³è‰²æ˜ å°„
        voice_map = {
            "ç”·å£°1": "M1",
            "ç”·å£°2": "M2",
            "å¥³å£°1": "F1",
            "å¥³å£°2": "F2"
        }
        
        text_1 = æ–‡æœ¬1
        text_2 = æ–‡æœ¬2
        text_3 = æ–‡æœ¬3
        voice_style_1 = voice_map.get(éŸ³è‰²1, éŸ³è‰²1)
        voice_style_2 = voice_map.get(éŸ³è‰²2, éŸ³è‰²2)
        voice_style_3 = voice_map.get(éŸ³è‰²3, éŸ³è‰²3)
        total_steps = æŽ¨ç†æ­¥æ•°
        speed = è¯­é€Ÿå€æ•°
        use_gpu = ä½¿ç”¨GPU
        
        # æ”¶é›†éžç©ºæ–‡æœ¬åŠå…¶éŸ³è‰²
        texts = [text_1]
        voice_styles = [voice_style_1]
        
        if text_2.strip():
            texts.append(text_2)
            voice_styles.append(voice_style_2)
        
        if text_3.strip():
            texts.append(text_3)
            voice_styles.append(voice_style_3)
        
        # åŠ è½½æ¨¡åž‹
        onnx_dir = os.path.join(current_dir, "assets", "onnx")
        
        if not os.path.exists(onnx_dir):
            raise FileNotFoundError(
                f"ONNX æ¨¡åž‹æœªæ‰¾åˆ°: {onnx_dir}\n"
                "è¯·ä¸‹è½½æ¨¡åž‹: git clone https://huggingface.co/Supertone/supertonic assets"
            )
        
        if self.text_to_speech is None or self.current_model_dir != onnx_dir:
            self.text_to_speech = load_text_to_speech(onnx_dir, use_gpu)
            self.current_model_dir = onnx_dir
        
        # åŠ è½½éŸ³è‰²æ–‡ä»¶
        voice_style_paths = []
        for vs in voice_styles:
            path = os.path.join(current_dir, "assets", "voice_styles", f"{vs}.json")
            if not os.path.exists(path):
                raise FileNotFoundError(f"éŸ³è‰²æ–‡ä»¶ '{vs}' æœªæ‰¾åˆ°: {path}")
            voice_style_paths.append(path)
        
        style = load_voice_style(voice_style_paths, verbose=False)
        
        # æ‰¹é‡ç”Ÿæˆè¯­éŸ³
        import time
        start_time = time.time()
        print(f"[Supertonic TTS Batch] æ‰¹é‡ç”Ÿæˆ {len(texts)} æ®µæ–‡æœ¬")
        
        wav, duration = self.text_to_speech.batch(texts, style, total_steps, speed)
        
        # Convert to ComfyUI audio format (list of audio dicts)
        audio_outputs = []
        for i in range(len(texts)):
            wav_trimmed = wav[i, :int(self.text_to_speech.sample_rate * duration[i].item())]
            
            # éŸ³é¢‘å½’ä¸€åŒ–å¤„ç†
            max_val = np.abs(wav_trimmed).max()
            if max_val > 1.0:
                wav_trimmed = wav_trimmed / max_val
            elif max_val < 0.01:
                wav_trimmed = wav_trimmed / max_val * 0.5
            
            waveform = torch.from_numpy(wav_trimmed).float().unsqueeze(0).unsqueeze(0)
            audio_output = {
                "waveform": waveform,  # [1, 1, samples] - mono audio
                "sample_rate": self.text_to_speech.sample_rate
            }
            audio_outputs.append(audio_output)
        
        elapsed_time = time.time() - start_time
        print(f"[Supertonic TTS Batch] æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return (audio_outputs,)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "SupertonicTTS": SupertonicTTS,
    "SupertonicBatchTTS": SupertonicBatchTTS,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SupertonicTTS": "ðŸŽ™ï¸ Supertonic è¯­éŸ³åˆæˆ",
    "SupertonicBatchTTS": "ðŸŽ™ï¸ Supertonic æ‰¹é‡è¯­éŸ³åˆæˆ",
}
